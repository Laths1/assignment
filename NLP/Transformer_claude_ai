"""
Complete WCST Transformer Training Script
Optimized for Kaggle - Copy this entire file and run

Key features:
- Multi-trial context for in-context learning
- Memory-efficient data generation
- Gradient accumulation for large effective batch sizes
- Proper masked loss computation
- Progress tracking and checkpointing
"""

import math
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import gc
import sys
import os

# =============================================================================
# CONFIGURATION
# =============================================================================

# Device setup
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"Using CUDA: {torch.cuda.get_device_name(0)}")
    mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"GPU Memory: {mem_gb:.1f} GB")
else:
    device = torch.device("cpu")
    print("WARNING: Using CPU - training will be very slow!")

# Constants
SEP = 68
EOS = 69
LABEL_BASE = 64

# Hyperparameters - ADJUST THESE FOR YOUR KAGGLE GPU
# For K80 (12GB): Use these settings
# For P100 (16GB): Can increase training_size to 50k, batch_size to 64
# For T4/V100 (16GB+): Can use full settings below
TRAINING_SIZE = 30_000
NUM_CONTEXT_TRIALS = 4
BATCH_SIZE = 32
ACCUMULATION_STEPS = 2  # Effective batch = 32 * 2 = 64

VOCAB_SIZE = 70
D_MODEL = 256
NUM_HEADS = 8
NUM_LAYERS = 6
D_FF = 1024
DROPOUT = 0.1

LEARNING_RATE = 1e-4
EPOCHS = 60
PATIENCE = 10
WEIGHT_DECAY = 0.01

# Data splits
TRAIN_SPLIT = 0.7
VAL_SPLIT = 0.15
TEST_SPLIT = 0.15

CONTEXT_SWITCH_INTERVAL = 10_000

# =============================================================================
# DATA GENERATION
# =============================================================================

class MultiTrialWCST:
    """
    Generates multi-trial sequences for WCST task.
    Each sequence has K context trials (with answers) + 1 question trial.
    """
    def __init__(self, num_context_trials=4, context_switch_interval=10000):
        self.colours = ['red','blue','green','yellow']
        self.shapes = ['circle','square','star','cross']
        self.quantities = ['1','2','3','4']
        self.categories = ['C1','C2','C3','C4']
        self.cards = self._build_cards()
        self.num_context_trials = int(num_context_trials)
        self.context_switch_interval = int(context_switch_interval)
        self.rule_feature = 0  # 0=colour, 1=shape, 2=quantity
        self._seq_counter = 0

    def _build_cards(self):
        """Build deck of 64 cards"""
        cards = []
        for colour in self.colours:
            for shape in self.shapes:
                for quantity in self.quantities:
                    cards.append((colour, shape, quantity))
        return np.array(cards)

    @staticmethod
    def card_features(idx):
        """Extract (colour, shape, quantity) features from card index"""
        qty = idx % 4
        shp = (idx // 4) % 4
        col = (idx // 16) % 4
        return (col, shp, qty)

    def _category_cards_for_rule(self, rule_feat):
        """Get 4 category cards representing each feature value"""
        cat_cards = []
        for fval in range(4):
            for idx in range(64):
                f = self.card_features(idx)
                if f[rule_feat] == fval:
                    cat_cards.append(idx)
                    break
        return cat_cards

    def _label_for_example(self, rule_feat, example_idx):
        """Determine which category (0-3) the example belongs to"""
        f = self.card_features(example_idx)
        return f[rule_feat]

    def _one_trial_segment(self, rule_feat):
        """
        Generate one trial segment:
        [cat0, cat1, cat2, cat3, example_card, SEP, label, EOS]
        """
        cats = self._category_cards_for_rule(rule_feat)
        ex_card = np.random.randint(0, 64)
        label_val = self._label_for_example(rule_feat, ex_card)
        label_tok = LABEL_BASE + label_val
        seg = cats + [ex_card, SEP, label_tok, EOS]
        return seg

    def _question_segment(self, rule_feat):
        """
        Generate question trial (includes answer for training):
        [cat0, cat1, cat2, cat3, question_card, SEP, label, EOS]
        """
        cats = self._category_cards_for_rule(rule_feat)
        q_card = np.random.randint(0, 64)
        label_val = self._label_for_example(rule_feat, q_card)
        label_tok = LABEL_BASE + label_val
        seg = cats + [q_card, SEP, label_tok, EOS]
        return seg, label_tok

    def gen_sequence(self):
        """
        Generate complete multi-trial sequence.
        Returns:
            seq: List of token IDs
            sep_mask: Mask for positions after SEP (where we compute loss)
            final_label: Label of the question trial (for final accuracy)
        """
        # Context switch (change rule periodically)
        if self._seq_counter != 0 and (self._seq_counter % self.context_switch_interval == 0):
            choices = [0, 1, 2]
            choices.remove(self.rule_feature)
            self.rule_feature = np.random.choice(choices)

        rule_feat = self.rule_feature
        seq = []
        
        # Generate K context trials with answers
        for _ in range(self.num_context_trials):
            seg = self._one_trial_segment(rule_feat)
            seq += seg

        # Generate question trial
        qseg, q_label = self._question_segment(rule_feat)
        seq += qseg

        # Create mask for loss computation (1 at positions after SEP)
        sep_mask = [0] * len(seq)
        for i in range(1, len(seq)):
            if seq[i-1] == SEP:
                sep_mask[i] = 1

        self._seq_counter += 1
        return seq, sep_mask, q_label


class MultiTrialDatasetLoader:
    """Loads and splits multi-trial WCST data"""
    def __init__(self, training_size, train_split=0.7, val_split=0.15, test_split=0.15,
                 num_context_trials=4, context_switch_interval=10000, batch_size=64):
        self.training_size = training_size
        self.train_split = train_split
        self.val_split = val_split
        self.test_split = test_split
        self.num_context_trials = num_context_trials
        self.context_switch_interval = context_switch_interval
        self.batch_size = batch_size

    def load_data(self):
        """Generate and split data into train/val/test"""
        print(f"\nGenerating {self.training_size} sequences...")
        print(f"Context trials per sequence: {self.num_context_trials}")
        
        env = MultiTrialWCST(
            num_context_trials=self.num_context_trials,
            context_switch_interval=self.context_switch_interval
        )

        seqs = []
        masks = []
        finals = []
        
        # Progress tracking
        checkpoint = max(1, self.training_size // 20)
        
        for i in range(self.training_size):
            if i % checkpoint == 0:
                pct = 100 * i / self.training_size
                print(f"  Progress: {i}/{self.training_size} ({pct:.1f}%)")
                sys.stdout.flush()
                
                # Periodic garbage collection
                if i > 0 and i % (checkpoint * 5) == 0:
                    gc.collect()
            
            s, m, f = env.gen_sequence()
            seqs.append(s)
            masks.append(m)
            finals.append(f)

        print("  Padding sequences...")
        max_len = max(len(s) for s in seqs)
        print(f"  Max sequence length: {max_len}")
        
        def pad(arr, pad_value):
            return [x + [pad_value] * (max_len - len(x)) for x in arr]

        # Create input/target pairs for next-token prediction
        input_ids = pad([s[:-1] for s in seqs], EOS)
        target_ids = pad([s[1:] for s in seqs], EOS)
        sep_mask = pad([m[1:] for m in masks], 0)
        
        # Create mask for final trial only (for reporting final accuracy)
        final_mask = []
        for m in sep_mask:
            idx = len(m) - 1 - next((i for i, v in enumerate(reversed(m)) if v == 1), 0)
            fm = [0] * len(m)
            fm[idx] = 1
            final_mask.append(fm)

        print("  Converting to tensors...")
        X = torch.tensor(input_ids, dtype=torch.long)
        Y = torch.tensor(target_ids, dtype=torch.long)
        M = torch.tensor(sep_mask, dtype=torch.bool)
        F = torch.tensor(final_mask, dtype=torch.bool)

        # Clear intermediate data
        del seqs, masks, finals, input_ids, target_ids, sep_mask, final_mask
        gc.collect()

        # Split into train/val/test
        n = X.size(0)
        n_train = int(self.train_split * n)
        n_val = int(self.val_split * n)
        idx = np.arange(n)
        np.random.shuffle(idx)

        X = X[idx]; Y = Y[idx]; M = M[idx]; F = F[idx]
        
        Xtr, Ytr, Mtr, Ftr = X[:n_train], Y[:n_train], M[:n_train], F[:n_train]
        Xva, Yva, Mva, Fva = X[n_train:n_train+n_val], Y[n_train:n_train+n_val], M[n_train:n_train+n_val], F[n_train:n_train+n_val]
        Xte, Yte, Mte, Fte = X[n_train+n_val:], Y[n_train+n_val:], M[n_train+n_val:], F[n_train+n_val:]

        train = TensorDataset(Xtr, Ytr, Mtr, Ftr)
        val = TensorDataset(Xva, Yva, Mva, Fva)
        test = TensorDataset(Xte, Yte, Mte, Fte)

        print(f"  Train: {len(train)}, Val: {len(val)}, Test: {len(test)}")
        return train, val, test, max_len


# =============================================================================
# MODEL ARCHITECTURE
# =============================================================================

class TokenEmbedding(nn.Module):
    """Standard token embedding with scaling"""
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, d_model)
        self.scale = math.sqrt(d_model)
    
    def forward(self, x):
        return self.emb(x) * self.scale


class PositionalEncoder(nn.Module):
    """Sinusoidal positional encoding"""
    def __init__(self, d_model, max_len=2048):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0)/d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        self.register_buffer("pe", pe)
    
    def forward(self, x):
        T = x.size(1)
        return self.pe[:T].unsqueeze(0)


class MultiHeadAttention(nn.Module):
    """Multi-head self-attention with causal masking"""
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0
        self.h = num_heads
        self.dh = d_model // num_heads
        self.Wq = nn.Linear(d_model, d_model, bias=True)
        self.Wk = nn.Linear(d_model, d_model, bias=True)
        self.Wv = nn.Linear(d_model, d_model, bias=True)
        self.Wo = nn.Linear(d_model, d_model, bias=True)
        self.drop = nn.Dropout(dropout)

    def _shape(self, x):
        B, T, C = x.shape
        return x.view(B, T, self.h, self.dh).transpose(1, 2)

    def forward(self, x, causal=True):
        q = self._shape(self.Wq(x))
        k = self._shape(self.Wk(x))
        v = self._shape(self.Wv(x))
        
        logits = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.dh)

        # Causal mask
        if causal:
            T = x.size(1)
            i = torch.arange(T, device=x.device).unsqueeze(1)
            j = torch.arange(T, device=x.device).unsqueeze(0)
            mask = j <= i
            logits = logits.masked_fill(~mask.unsqueeze(0).unsqueeze(0), float('-inf'))

        w = torch.softmax(logits, dim=-1)
        w = self.drop(w)
        ctx = torch.matmul(w, v)
        ctx = ctx.transpose(1, 2).contiguous().view(x.size(0), x.size(1), -1)
        out = self.Wo(ctx)
        return out, w


class FeedForward(nn.Module):
    """Position-wise feed-forward network"""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.act = nn.GELU()
        self.drop = nn.Dropout(dropout)
        self.fc2 = nn.Linear(d_ff, d_model)
    
    def forward(self, x):
        return self.fc2(self.drop(self.act(self.fc1(x))))


class DecoderLayer(nn.Module):
    """Single transformer decoder layer"""
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.ln2 = nn.LayerNorm(d_model)
        self.ff = FeedForward(d_model, d_ff, dropout)
        self.drop = nn.Dropout(dropout)
    
    def forward(self, x):
        h, _ = self.attn(self.ln1(x), causal=True)
        x = x + self.drop(h)
        x = x + self.drop(self.ff(self.ln2(x)))
        return x


class TransformerDecoderOnly(nn.Module):
    """Decoder-only transformer for WCST"""
    def __init__(self, num_layers, d_model, num_heads, d_ff, 
                 vocab_size, max_len=1024, dropout=0.1):
        super().__init__()
        self.emb = TokenEmbedding(vocab_size, d_model)
        self.pos = PositionalEncoder(d_model, max_len)
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        self.fc_out = nn.Linear(d_model, vocab_size, bias=False)
        self.drop = nn.Dropout(dropout)

    def forward(self, x):
        h = self.emb(x) + self.pos(x).to(x.device)
        h = self.drop(h)
        for lyr in self.layers:
            h = lyr(h)
        return self.fc_out(h)


# =============================================================================
# TRAINING
# =============================================================================

class ImprovedWCSTTrainer:
    """Trainer with masked loss and gradient accumulation"""
    def __init__(self, model, train_data, val_data, test_data,
                 lr=1e-4, batch_size=64, patience=10, epochs=100,
                 weight_decay=0.01, accumulation_steps=1):
        self.model = model.to(device)
        self.batch_size = batch_size
        self.accumulation_steps = accumulation_steps
        
        self.train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
        self.val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)
        self.test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

        self.crit = nn.CrossEntropyLoss(reduction='none')
        self.opt = optim.AdamW(self.model.parameters(), lr=lr, 
                               weight_decay=weight_decay, betas=(0.9, 0.98), eps=1e-8)
        self.patience = patience
        self.epochs = epochs
        self.best_val = float('inf')
        self.no_improve = 0

    @staticmethod
    def _masked_loss(logits, targets, mask):
        """Compute loss only at masked positions (after SEP tokens)"""
        B, T, V = logits.shape
        loss_all = nn.functional.cross_entropy(
            logits.reshape(B*T, V),
            targets.reshape(B*T),
            reduction='none'
        ).reshape(B, T)
        loss = (loss_all * mask.float()).sum() / (mask.float().sum().clamp_min(1.0))
        return loss

    @staticmethod
    def _masked_accuracy(logits, targets, mask):
        """Accuracy at all masked positions"""
        pred = logits.argmax(dim=-1)
        correct = ((pred == targets) & mask).sum().item()
        total = mask.sum().item()
        return correct / total if total > 0 else 0.0

    @staticmethod
    def _final_accuracy(logits, targets, final_mask):
        """Accuracy at final trial only (what we care about most)"""
        pred = logits.argmax(dim=-1)
        correct = ((pred == targets) & final_mask).sum().item()
        total = final_mask.sum().item()
        return correct / total if total > 0 else 0.0

    def _run_epoch(self, loader, train):
        """Run one epoch of training or evaluation"""
        if train:
            self.model.train()
        else:
            self.model.eval()

        total_loss = 0.0
        total_masked_acc = 0.0
        total_final_acc = 0.0
        batches = 0

        # Progress tracking
        num_batches = len(loader)
        print_every = max(1, num_batches // 10)

        if train:
            self.opt.zero_grad()

        for batch_idx, (X, Y, M, F) in enumerate(loader):
            X, Y, M, F = X.to(device), Y.to(device), M.to(device), F.to(device)
            
            with torch.set_grad_enabled(train):
                logits = self.model(X)
                loss = self._masked_loss(logits, Y, M)
                
                if train:
                    # Gradient accumulation
                    loss = loss / self.accumulation_steps
                    loss.backward()
                    
                    if (batch_idx + 1) % self.accumulation_steps == 0:
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                        self.opt.step()
                        self.opt.zero_grad()

            masked_acc = self._masked_accuracy(logits, Y, M)
            final_acc = self._final_accuracy(logits, Y, F)

            total_loss += loss.item() * self.accumulation_steps
            total_masked_acc += masked_acc
            total_final_acc += final_acc
            batches += 1

            # Progress update
            if train and (batch_idx + 1) % print_every == 0:
                avg_loss = loss.item() * self.accumulation_steps
                print(f"  Batch {batch_idx+1}/{num_batches} | "
                      f"Loss: {avg_loss:.4f} | Acc: {final_acc:.3f}")
                sys.stdout.flush()

            # Memory cleanup
            if torch.cuda.is_available() and batch_idx % 50 == 0:
                torch.cuda.empty_cache()

        return (total_loss / batches,
                total_masked_acc / batches,
                total_final_acc / batches)

    def train(self):
        """Full training loop"""
        print("\n" + "="*60)
        print("TRAINING START")
        print("="*60)
        
        for ep in range(1, self.epochs+1):
            print(f"\nEpoch {ep}/{self.epochs}")
            print("-"*60)
            
            tr_loss, tr_macc, tr_facc = self._run_epoch(self.train_loader, train=True)
            va_loss, va_macc, va_facc = self._run_epoch(self.val_loader, train=False)

            print(f"\nEpoch {ep:03d} Summary:")
            print(f"  Train: loss={tr_loss:.4f}, masked_acc={tr_macc:.3f}, final_acc={tr_facc:.3f}")
            print(f"  Val:   loss={va_loss:.4f}, masked_acc={va_macc:.3f}, final_acc={va_facc:.3f}")

            if va_loss + 1e-6 < self.best_val:
                self.best_val = va_loss
                self.no_improve = 0
                torch.save(self.model.state_dict(), "best_wcst_multi.pth")
                print("  ✓ New best model saved!")
            else:
                self.no_improve += 1
                print(f"  No improvement for {self.no_improve} epoch(s)")
                if self.no_improve >= self.patience:
                    print(f"\n⚠ Early stopping at epoch {ep}")
                    break

            # Memory cleanup
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

        # Final test evaluation
        print("\n" + "="*60)
        print("FINAL TEST EVALUATION")
        print("="*60)
        self.model.load_state_dict(torch.load("best_wcst_multi.pth"))
        te_loss, te_macc, te_facc = self._run_epoch(self.test_loader, train=False)
        print(f"\nTEST Results:")
        print(f"  Loss: {te_loss:.4f}")
        print(f"  Masked Accuracy: {te_macc:.3f}")
        print(f"  Final Accuracy: {te_facc:.3f}")
        
        if te_facc > 0.75:
            print("\n✓ Excellent performance (>75%)!")
        elif te_facc > 0.6:
            print("\n✓ Good performance (60-75%)")
        elif te_facc > 0.4:
            print("\n⚠ Moderate performance (40-60%) - consider training longer")
        else:
            print("\n✗ Low performance (<40%) - check hyperparameters")
        
        print("="*60)


# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    print("="*60)
    print("WCST TRANSFORMER TRAINING")
    print("="*60)
    
    print(f"\nConfiguration:")
    print(f"  Training size: {TRAINING_SIZE}")
    print(f"  Context trials: {NUM_CONTEXT_TRIALS}")
    print(f"  Batch size: {BATCH_SIZE} (accumulation: {ACCUMULATION_STEPS}x)")
    print(f"  Effective batch: {BATCH_SIZE * ACCUMULATION_STEPS}")
    print(f"  Model: d_model={D_MODEL}, heads={NUM_HEADS}, layers={NUM_LAYERS}")
    print(f"  Learning rate: {LEARNING_RATE}")
    print(f"  Device: {device}")
    
    # Generate data
    loader = MultiTrialDatasetLoader(
        training_size=TRAINING_SIZE,
        train_split=TRAIN_SPLIT,
        val_split=VAL_SPLIT,
        test_split=TEST_SPLIT,
        num_context_trials=NUM_CONTEXT_TRIALS,
        context_switch_interval=CONTEXT_SWITCH_INTERVAL,
        batch_size=BATCH_SIZE
    )
    train_data, val_data, test_data, max_len = loader.load_data()
    
    print(f"\n✓ Data loaded successfully")
    
    # Build model
    print("\nBuilding model...")
    model = TransformerDecoderOnly(
        num_layers=NUM_LAYERS,
        d_model=D_MODEL,
        num_heads=NUM_HEADS,
        d_ff=D_FF,
        vocab_size=VOCAB_SIZE,
        max_len=max_len+1,
        dropout=DROPOUT
    ).to(device)
    
    # Weight tying
    model.fc_out.weight = model.emb.emb.weight
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  Total parameters: {total_params:,}")
    print(f"  Model size: ~{total_params * 4 / 1e6:.1f} MB")
    
    # Train
    trainer = ImprovedWCSTTrainer(
        model=model,
        train_data=train_data,
        val_data=val_data,
        test_data=test_data,
        lr=LEARNING_RATE,
        batch_size=BATCH_SIZE,
        patience=PATIENCE,
        epochs=EPOCHS,
        weight_decay=WEIGHT_DECAY,
        accumulation_steps=ACCUMULATION_STEPS
    )
    trainer.train()
    
    print("\n✓ Training complete!")
    print(f"Best model saved to: best_wcst_multi.pth")


if __name__ == "__main__":
    main()
